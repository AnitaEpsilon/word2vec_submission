{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вступление"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда бывает полезно уметь оценить, насколько два слова близки друг другу по смыслу. Для этого можно поставить каждому слову в соответствие какое-нибудь число или набор чисел (то есть вектор) - и определять расстояние уже между числами, а не словами. \n",
    "\n",
    "Первая идея, появившаяся у человечества по этому поводу - составить список всех слов, а затем в качестве присвоенного слову числа брать его номер в этом списке. Но у этого метода есть существенный недостаток. Иногда стоящие рядом в словаре слова действительно оказываются семантически близки (\"учитель\", \"учительница\"), но часто - это не так (к слову \"бумага\" по смыслу более близко слово \"картон\", а по списку - бумеранг\").\n",
    "\n",
    "Данная проблема была решена в 2013 году, когда чешский аспирант Томаш Миколов предложил за параметр \"близости\" слов принимать вероятность встретить их стоящими рядом в тексте. Это и есть основная идея метода word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re   # Импортировали библиотеку \n",
    "            # для использования регулярных выражений\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем перейти к обучению модели (которой ещё нет, но скоро будет) на тексте, его нужно к этому подготовить:\n",
    "    \n",
    "    1. Провести токенизацию(т. е. разбить на отдельные слова)\n",
    "    2. Удалить пунктуационные символы\n",
    "    3. Привести все слова к нижнему регистру \n",
    "    \n",
    "Это всё делает описанная ниже функция:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generate(file):\n",
    "    \n",
    "    f = open(file, 'r')          # Открыли текстовый файл\n",
    "    arr = []                     # Создали массив для последовательной записи слов текста\n",
    "    \n",
    "    for line in f:\n",
    "        s = line\n",
    "        s = s.lower()            # Привели все строчки к нижнему регистру \n",
    "                                 # (это понизит количество слов в словаре без ротери данных)\n",
    "        \n",
    "        spl = re.split('[^a-z]', s)\n",
    "        k = spl.count('')\n",
    "        for i in range(k):\n",
    "            spl.remove('')       # Разбили строки на слова и удалили все пунктуационные символы\n",
    "       \n",
    "        for i in range(len(spl)):\n",
    "            arr.append(spl[i])   # Заполнили массив словами текста (последовательно)        \n",
    "     \n",
    "    return(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь применим функцию к тексту, который мы будем использовать - отрывок из книги Марка Твена \"Приключения Тома Сойера\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = data_generate('training_text.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы проверить, как сработала функция, посмотрим на первые 10 слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom', 'joined', 'the', 'new', 'order', 'of', 'cadets', 'of', 'temperance', 'being']\n"
     ]
    }
   ],
   "source": [
    "print words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, функция действительно работает - слова разбиты по-одному, пробелы и символы убраны, регистр везде нижний. Можно двигаться дальше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы было удобнее обращаться к словам, создадим два словаря. В одном из них каждому слову будет присвоен порядковый номер, а в другом - пары ключ-значение будут поменяны местами (т. е. каждому номеру поставлено в соответствие слово). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_words = {}\n",
    "d_indexes = {}\n",
    "\n",
    "for i, word in enumerate(set(words)):\n",
    "    d_words[word] = i\n",
    "    d_indexes[i] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на количество слов в словаре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457\n"
     ]
    }
   ],
   "source": [
    "voc_size = len(d_words)\n",
    "print voc_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно заняться созданием элементов данных. Выберем размер контекстного окна window_size (это гиперпараметр, то есть параметр, который нельзя извлечь из самого процесса обучения). Для каждого слова будем определять контекстное окно размером window_size вправо и влево от целевого слова, а затем будем записывать все пары \"целевое слово - слово из контекстного окна\". Эти пары и будут нужными нам элементами данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова из каждой пары мы будем записывать в два отдельных массива (точнее, вектора) в виде их индексов в словаре. В векторе X будут лежать целевые слова. Это одномерная матрица признаков, по которым модель должна предсказывать вероятность появления контекстных слов. В вектор Y положим контекстные слова - они будут ответами на обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(len(words)):\n",
    "    for j in range(1, window_size+1):\n",
    "        \n",
    "        if i - j >= 0:\n",
    "          \n",
    "            X.append(d_words[words[i]])\n",
    "            Y.append(d_words[words[i-j]])\n",
    "            \n",
    "            \n",
    "        if i + j <= len(words)-1:\n",
    "           \n",
    "            X.append(d_words[words[i]])\n",
    "            Y.append(d_words[words[i+j]])\n",
    "            \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6258\n"
     ]
    }
   ],
   "source": [
    "n = len(Y)\n",
    "print n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего в обучающей выборке n пар.\n",
    "Посмотрим на первые 10 значений каждого вектора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[185 185 185  28  28  28  28 336 336 336]\n",
      "[ 28 336  39 185 336  39 110  28  39 185]\n"
     ]
    }
   ],
   "source": [
    "print X[:10]\n",
    "print Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё в порядке - код сработал верно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждое слово можно рассматривать как отдельный класс. Представим слово как вектор длины voc_size. На позиции, равной индексу слова в словаре, будет стоять 1, а на всех остальнх позициях - 0 (заметим, что при этом сумма чисел в векторе равна единице и у нас получается аналогия стопроцентной вероятности). Подобное представление получило название one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(y):\n",
    "    \n",
    "    n = len(y)\n",
    "    hot = np.zeros((voc_size, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        hot[y[i], i] = 1\n",
    "    \n",
    "    hot = hot.astype(np.int64)\n",
    "    \n",
    "    return hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hot = one_hot_encoder(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом мы заканчиваем препроцессинг данных и переходим собственно к построению модели. Однако стоит отметить, что мы сделали далеко не всё, что можно было сделать. Напрмер, слишком часто встречающиеся слова (\"a\", \"the\", \"and\" etc) можно не принимать в рассчёт, поскольку они могут попасть в контекстное окно практически с любым словом. Аналогично с очень редко используемыми словами - для них будет слишком маленькая выборка контекстных слов, по которой нельзя построить хорошую зависимость. Но чтобы убирать такие слова, пришлось бы подбирать ещё два гиперпараметра - предельные (верхняя и нижняя) частоты встречаемости. Я не стала этого делать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе работы программы мы хотим получить матрицу, задающую векторное представление для каждого слова, причём таким образом, что чем чаще пара слов встречаются в одном контекстном окне, тем ближе друг к другу точки, описываемые этими векторами. Следовательно, у нас появился ещё один гиперпараметр - размерность пространства слов (emb_size, от embedding - прикрепляющий, запечатляющий). Если взять параметр emb_size слишком маленьким, то модель будет плохо описывать реальность, а если слишком большим - долго обучаться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернемся к матрице, получаемой на выходе. Она имеет размеры (voc_size, emb_size), чтобы каждому слову в словаре соответствовал вектор. Для начала инициализируем эту матрицу случайными значениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb = np.random.sample((voc_size, emb_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также наша модель будет иметь один скрытый слой - dense_layer. На вход этому слою будут подаваться веторные представления слов из вектора X, которых всего n штук. То есть на вход dense_layer получает матрицу размера (emb_size, n). На выходе скрытый слой должен давать вероятности того, что каждое заданное слово (которых всего n) окажется в паре с каждым словом из словаря (а их voc_size штук) - таким образом на выходе мы ожидаем матрицу размера (voc_size, n). Так как в скрытом слое будет проходить операция матричного умножения, он тоже должен иметь размеры (voc_size, emb_size). \n",
    "\n",
    "Инициализируем dense_layer случайными значениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = np.random.sample((voc_size, emb_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заведём словарь для хранения тренируемых параметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "weights[\"word_emb\"] = word_emb\n",
    "weights['dense_layer'] = dense_layer\n",
    "weights[\"X\"] = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прямое распространение ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый этап в каждом цикле обучения - прямое распространение ошибки (англ. forward propagation). \n",
    "\n",
    "На вход программы подаются слова, которые прежде всего нужно превратить в векторы. Векторные представления слов хранятся в массиве word_emb словаря weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_representations(X, weights=weights):\n",
    "    \n",
    "    word_emb = weights['word_emb']\n",
    "    word_vec = word_emb[X.flatten(), :].T\n",
    "    weights['word_vec'] = word_vec\n",
    "    \n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Написанная выше функция возвращает матрицу, у которой в каждой строке закодированы emb_size-мерными векторами подаваемые на вход слова. Применим функцию к ветору целевых слов X и посмотрим на первые 5 элементов получившейся матрицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2924399  0.2924399  0.2924399  ... 0.99513672 0.99513672 0.99513672]\n",
      " [0.47131404 0.47131404 0.47131404 ... 0.67305381 0.67305381 0.67305381]\n",
      " [0.72300482 0.72300482 0.72300482 ... 0.36731023 0.36731023 0.36731023]\n",
      " [0.72484276 0.72484276 0.72484276 ... 0.59013117 0.59013117 0.59013117]\n",
      " [0.91495268 0.91495268 0.91495268 ... 0.34057055 0.34057055 0.34057055]]\n"
     ]
    }
   ],
   "source": [
    "word_vec_0 = vector_representations(X, weights=weights)\n",
    "print word_vec_0[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё работает.\n",
    "\n",
    "Следующий шаг - перемножить получившуюся матрицу с тренируемой матрицей dense_layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer_multiplication(word_vec, weights=weights):\n",
    "    \n",
    "    dense_layer = weights['dense_layer']\n",
    "    Z = np.dot(dense_layer, word_vec)\n",
    "    \n",
    "    return dense_layer, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результат работы функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.09360033 3.09360033 3.09360033 ... 3.95876509 3.95876509 3.95876509]\n",
      " [2.24916844 2.24916844 2.24916844 ... 3.18112802 3.18112802 3.18112802]\n",
      " [3.44456342 3.44456342 3.44456342 ... 5.25263275 5.25263275 5.25263275]\n",
      " [1.72565389 1.72565389 1.72565389 ... 2.98144892 2.98144892 2.98144892]\n",
      " [1.49313057 1.49313057 1.49313057 ... 1.60964818 1.60964818 1.60964818]]\n"
     ]
    }
   ],
   "source": [
    "dense_layer_0, Z_0 = dense_layer_multiplication(word_vec_0, weights=weights)\n",
    "print Z_0[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И, наконец, финальный шаг в forward propagation - применить к полученной матрице функцию softmax. Это приведет к тому, что каждая координата матрицы будет лежать на отрезке [0, 1], отношения между координатами одной строки не изменятся, а построчные суммы координат будут равняться единице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    return np.exp(Z)/np.sum(np.exp(Z), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00284139 0.00284139 0.00284139 ... 0.00263078 0.00263078 0.00263078]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "softmax_out_0 = softmax(Z_0)\n",
    "\n",
    "print softmax_out_0[0]\n",
    "print np.sum(softmax_out_0[:], axis=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обратное распространение ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этапе backward propagation происходит подсчёт ошибки и правка тренируемых параметров. Для подсчёта ошибки возьмём функцию Cross Entropy Loss - её обычно используют для вероятностных параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Z, y):\n",
    "  \n",
    "    y_1 = y.argmax(axis=1)\n",
    "    p = softmax(Z)\n",
    "    m = y_1.shape[0]\n",
    "    log_likelihood = np.log(p[range(m),y_1])\n",
    "    loss = -np.sum(log_likelihood)/m\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какая ошибка получилась у нас после первой итерации forward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.266295757386325\n"
     ]
    }
   ],
   "source": [
    "loss_0 = cross_entropy(Z_0, Y_hot)\n",
    "print loss_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка получилась неприемлемо большой, что не удивительно - параметры для первой итерации подбирались случайным образом.\n",
    "\n",
    "На этапе backward propagation будем определять, в каком направлении ошибка убывает быстрее всего, после чего будем двигаться туда. Это направление задаёт антиградиент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь перйдём к самой функции обратного распространения ошибки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(Y_hot, softmax_out, weights, learning_rate=0.01):\n",
    "    \n",
    "    \n",
    "    dL_dZ = softmax_out - Y_hot             # Насколько предсказанные вероятности\n",
    "                                            # отличаются от истинного значения\n",
    "    \n",
    "    \n",
    "    word_vec = weights['word_vec']           # Извлекаем из словаря нужные нам параметры\n",
    "    dense_layer = weights['dense_layer']\n",
    "    X = weights['X']\n",
    "    word_emb = weights['word_emb']\n",
    "    \n",
    "    n = word_vec.shape[1]\n",
    "    \n",
    "    dL_d_dense_layer = (1 / n) * np.dot(dL_dZ, word_vec.T)       # Считаем градиенты весов  \n",
    "    dL_dword_vec = np.dot(dense_layer.T, dL_dZ)\n",
    "    \n",
    "    \n",
    "    gradients = dict()                                      # Сохраним посчитанные градинты\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_d_dense_layer'] = dL_d_dense_layer\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    word_emb = weights['word_emb'] \n",
    "    word_emb[X.flatten(), :] -= dL_dword_vec.T * learning_rate                 # \"Сдвигаем\" значения весов на антиградиент\n",
    "    weights['dense_layer'] -= learning_rate * gradients['dL_d_dense_layer']    # Обновили веса модели\n",
    "                                                                               # (гиперпараметр learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тренировка модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда все необходимые функции написаны, можно перейти к тренировке модели. Вспомним, какие у нас есть данные:\n",
    "\n",
    "    X - индексы первых в паре слов\n",
    "    Y - индексы вторых в паре слов\n",
    "    Y_hot - вторые в паре слов, записанные с помощью one-hot code\n",
    "    emb_size = 10 - размерность пространства векторов\n",
    "    voc_size - количество слов в словаре"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим количество эпох, то есть то, за сколько итераций мы будем проводить обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(По-хорошему, следует делать порядка 1000 итераций, но с таким числом циклов программа работает невыносимо долго)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим процесс обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = weights['word_vec']\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "         \n",
    "        dense_layer, Z = dense_layer_multiplication(word_vec, weights=weights)\n",
    "        softmax_out = softmax(Z)\n",
    "        \n",
    "        backward_propagation(Y_hot, softmax_out, weights, learning_rate=0.01)\n",
    "    \n",
    "        cost = cross_entropy(softmax_out, Y_hot) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже представлена функция, которая для заданного слова подбираем наиболее на него похожее (по мнению программы, естественно). Чем меньше косинусное расстояние, тем более семантически близки слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word_emb, index):\n",
    "    \n",
    "    min = 10\n",
    "    nearest_index = -10\n",
    "    \n",
    "    for i in range(voc_size):\n",
    "        if scipy.spatial.distance.cosine(word_emb[i], word_emb[index]) < min and i != index:\n",
    "            \n",
    "            min = scipy.spatial.distance.cosine(word_emb[i], word_emb[index])\n",
    "            nearest_index = i\n",
    "            \n",
    "    print d_indexes[index], d_indexes[nearest_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на наиболее близкое слово, подобранное каждому слову из словаря:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all s\n",
      "impulse there\n",
      "forty band\n",
      "grateful they\n",
      "go wondered\n",
      "voids greatest\n",
      "seemed tracts\n",
      "dreadful accomplishing\n",
      "shackles they\n",
      "religion surest\n",
      "concerned hollis\n",
      "to drearier\n",
      "chewing profanity\n",
      "under trust\n",
      "melon venture\n",
      "suffered poor\n",
      "regalia gave\n",
      "scriptural weeks\n",
      "feebly an\n",
      "presently time\n",
      "very vacation\n",
      "horror desperation\n",
      "diary stolen\n",
      "surprise over\n",
      "awful such\n",
      "condition desperation\n",
      "entire abroad\n",
      "did no\n",
      "joined withdrawing\n",
      "dis nor\n",
      "neighborhood tom\n",
      "sensation everywhere\n",
      "second parents\n",
      "street lapsed\n",
      "tempest claps\n",
      "depression too\n",
      "even found\n",
      "deeply con\n",
      "above made\n",
      "new go\n",
      "ammunition an\n",
      "ever however\n",
      "public overwhelming\n",
      "body upon\n",
      "melancholy deathbed\n",
      "remem realizing\n",
      "mem thing\n",
      "funeral forbearance\n",
      "never grateful\n",
      "hours little\n",
      "alone dreadful\n",
      "re happened\n",
      "change worn\n",
      "wait took\n",
      "boy no\n",
      "withdrawing joined\n",
      "study fixed\n",
      "misery hands\n",
      "frazer sadly\n",
      "thatcher did\n",
      "everybody actual\n",
      "from afterward\n",
      "would disgusted\n",
      "two boys\n",
      "by three\n",
      "few ill\n",
      "doubt on\n",
      "injury failure\n",
      "more old\n",
      "glass hollis\n",
      "everywhere sensation\n",
      "broke twenty\n",
      "doom relapse\n",
      "town lost\n",
      "high side\n",
      "drearier to\n",
      "this supposed\n",
      "worn change\n",
      "cat nor\n",
      "forbearance been\n",
      "anywhere circus\n",
      "crossed bed\n",
      "mr between\n",
      "believed its\n",
      "heart de\n",
      "estate waste\n",
      "artillery back\n",
      "states huck\n",
      "want who\n",
      "sense mission\n",
      "sought cancer\n",
      "united received\n",
      "court spent\n",
      "mesmerizer took\n",
      "how when\n",
      "aching rag\n",
      "pins rag\n",
      "creature there\n",
      "becky without\n",
      "disgusted envy\n",
      "lay gave\n",
      "coming weeks\n",
      "such awful\n",
      "man dead\n",
      "a lads\n",
      "desperation hoping\n",
      "so hard\n",
      "crept added\n",
      "basket thing\n",
      "every fluctuating\n",
      "order disappointment\n",
      "over surprise\n",
      "soon about\n",
      "its days\n",
      "before expensive\n",
      "style so\n",
      "paraded constantinople\n",
      "jim chewing\n",
      "bosom harper\n",
      "late hands\n",
      "hubbub rogers\n",
      "covered visiting\n",
      "weeks night\n",
      "might hope\n",
      "happened re\n",
      "then attracted\n",
      "lonely made\n",
      "fourth however\n",
      "flew upon\n",
      "band claps\n",
      "they grateful\n",
      "hands misery\n",
      "not suffered\n",
      "hunted lightning\n",
      "now did\n",
      "day parties\n",
      "nor cat\n",
      "hollis glass\n",
      "delightful artillery\n",
      "found abandoned\n",
      "went hope\n",
      "side high\n",
      "duller he\n",
      "tents ever\n",
      "hard so\n",
      "out down\n",
      "huck states\n",
      "wondered go\n",
      "res lonely\n",
      "since parents\n",
      "discouraging drink\n",
      "looking desire\n",
      "attracted then\n",
      "acting dis\n",
      "showy deathbed\n",
      "got convalescent\n",
      "happenings melon\n",
      "red rogers\n",
      "shows moved\n",
      "upon way\n",
      "free news\n",
      "bering reform\n",
      "beginning tom\n",
      "afterward from\n",
      "minstrel refuge\n",
      "could tormented\n",
      "days ton\n",
      "times vacation\n",
      "thing victim\n",
      "testament shadow\n",
      "turf getting\n",
      "waited away\n",
      "first added\n",
      "one went\n",
      "feet ignation\n",
      "another hours\n",
      "little hours\n",
      "sadly frazer\n",
      "revival relapse\n",
      "twenty ran\n",
      "girls nothing\n",
      "their wait\n",
      "ton days\n",
      "too depression\n",
      "storm thatcher\n",
      "tom beginning\n",
      "phrenologist quotation\n",
      "murder forbearance\n",
      "pomp first\n",
      "that disgusted\n",
      "took wait\n",
      "negro gone\n",
      "procession constantinople\n",
      "doctors charm\n",
      "july battery\n",
      "overwhelming abroad\n",
      "than day\n",
      "grew parents\n",
      "ben swear\n",
      "performers gruous\n",
      "bed abroad\n",
      "supposed turf\n",
      "venture quotation\n",
      "were relapse\n",
      "ber swear\n",
      "powers surest\n",
      "and soon\n",
      "rag benton\n",
      "tracts about\n",
      "remained rain\n",
      "ran twenty\n",
      "turned knock\n",
      "alley so\n",
      "rained harder\n",
      "suspense alley\n",
      "have red\n",
      "spectacle promise\n",
      "apparently second\n",
      "any con\n",
      "abroad entire\n",
      "relapse were\n",
      "harper bosom\n",
      "terrific nothing\n",
      "permanency next\n",
      "blinding all\n",
      "play little\n",
      "added first\n",
      "pain con\n",
      "who face\n",
      "most him\n",
      "charm member\n",
      "sheets simple\n",
      "prisoner dreadful\n",
      "coveted bosom\n",
      "forever once\n",
      "rogers red\n",
      "failure injury\n",
      "calculated surest\n",
      "reform bering\n",
      "hungry in\n",
      "face who\n",
      "thunder doubt\n",
      "bedclothes depression\n",
      "came kept\n",
      "bright town\n",
      "finn joined\n",
      "fine dead\n",
      "find attention\n",
      "realizing regalia\n",
      "envy disgusted\n",
      "justice even\n",
      "storms time\n",
      "only battery\n",
      "mend age\n",
      "hope went\n",
      "do remem\n",
      "his tempest\n",
      "get character\n",
      "de heart\n",
      "mission sense\n",
      "during lapsed\n",
      "him most\n",
      "bird everywhere\n",
      "taxed in\n",
      "fixed do\n",
      "temperance insect\n",
      "namely hours\n",
      "result deathbed\n",
      "cadets an\n",
      "visiting covered\n",
      "forlorn victim\n",
      "away waited\n",
      "between mr\n",
      "listlessly delightful\n",
      "attention find\n",
      "however fourth\n",
      "spared to\n",
      "joe result\n",
      "news however\n",
      "precious hours\n",
      "come ignation\n",
      "received united\n",
      "last more\n",
      "profanity expensive\n",
      "ill ran\n",
      "against accomplishing\n",
      "s juvenile\n",
      "disappointment believed\n",
      "senator bedclothes\n",
      "glorious acting\n",
      "con deeply\n",
      "cancer sought\n",
      "simple sheets\n",
      "character get\n",
      "village it\n",
      "trust under\n",
      "expensive profanity\n",
      "adults for\n",
      "accomplishing dreadful\n",
      "vacation very\n",
      "been forbearance\n",
      "secret gruous\n",
      "dur circus\n",
      "waste estate\n",
      "hardly victim\n",
      "gruous secret\n",
      "life play\n",
      "measles court\n",
      "blessing rogers\n",
      "constantinople paraded\n",
      "smoking sequence\n",
      "quotation venture\n",
      "stolen diary\n",
      "abandoned found\n",
      "harder rained\n",
      "promised sash\n",
      "hoping desperation\n",
      "played re\n",
      "is was\n",
      "it artillery\n",
      "encountered every\n",
      "pressing some\n",
      "in felt\n",
      "claps band\n",
      "incon sadly\n",
      "make her\n",
      "member lads\n",
      "convalescent got\n",
      "drink sequence\n",
      "hang shackles\n",
      "rain in\n",
      "kept hungry\n",
      "itself every\n",
      "spent jim\n",
      "without surprise\n",
      "greatest pain\n",
      "the night\n",
      "left smoking\n",
      "abstain res\n",
      "heavily make\n",
      "being surest\n",
      "kill covered\n",
      "victim hardly\n",
      "huckleberry juvenile\n",
      "had chronic\n",
      "battery only\n",
      "parents since\n",
      "tormented could\n",
      "practise got\n",
      "gave regalia\n",
      "resolved street\n",
      "fluctuating overwhelming\n",
      "big died\n",
      "five since\n",
      "lads member\n",
      "judge delightful\n",
      "world long\n",
      "shadow testament\n",
      "desire looking\n",
      "knock promised\n",
      "like a\n",
      "lost town\n",
      "lightning another\n",
      "night weeks\n",
      "surest powers\n",
      "old more\n",
      "sequence too\n",
      "some sight\n",
      "back artillery\n",
      "dead man\n",
      "sight some\n",
      "home lay\n",
      "lapsed during\n",
      "pronounced withdrawing\n",
      "for adults\n",
      "everything next\n",
      "be stay\n",
      "object ad\n",
      "eating mission\n",
      "ache eight\n",
      "intense july\n",
      "refuge minstrel\n",
      "on doubt\n",
      "about soon\n",
      "actual withdrawing\n",
      "getting turf\n",
      "of that\n",
      "insect himself\n",
      "swear ber\n",
      "nothing girls\n",
      "eight when\n",
      "presence permanency\n",
      "blessed attempted\n",
      "three by\n",
      "down feet\n",
      "chronic rained\n",
      "promise spectacle\n",
      "parties day\n",
      "drifted sadly\n",
      "next permanency\n",
      "her coveted\n",
      "there impulse\n",
      "long world\n",
      "way upon\n",
      "was is\n",
      "happy dur\n",
      "head little\n",
      "himself insect\n",
      "attempted blessed\n",
      "but drink\n",
      "boys member\n",
      "hopes since\n",
      "trying desperation\n",
      "with taxed\n",
      "bug insect\n",
      "he ber\n",
      "made above\n",
      "extremity feet\n",
      "official played\n",
      "benton rag\n",
      "up mesmerizer\n",
      "companionless basket\n",
      "called pomp\n",
      "gone negro\n",
      "juvenile huckleberry\n",
      "proved sinful\n",
      "ad died\n",
      "sinful proved\n",
      "circus dur\n",
      "moved shows\n",
      "an ammunition\n",
      "as of\n",
      "warning sheets\n",
      "at bering\n",
      "ing abandoned\n",
      "again for\n",
      "circusing an\n",
      "no did\n",
      "endurance on\n",
      "peace second\n",
      "when so\n",
      "handed forbearance\n",
      "interested never\n",
      "carpeting happened\n",
      "ignation feet\n",
      "poor suffered\n",
      "felt in\n",
      "stay be\n",
      "chance swear\n",
      "deathbed result\n",
      "died big\n",
      "driving made\n",
      "age mend\n",
      "fact do\n",
      "time storms\n",
      "sash promised\n",
      "once forever\n"
     ]
    }
   ],
   "source": [
    "for i in range(voc_size):\n",
    "    similarity(word_emb, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что, хотя далеко не всегда слова получаются близкими по смыслу, среди получившихся пар есть весьма неплохо подобранные: \"suffered-poor\", \"presently-time\", \"melancholy-deathbed\", \"desperation-hoping\", \"weeks-night\", \"once-forever\", \"bug-insect\" etc (небольшая ремарка: перед начальной инициализацией параметров я забыла поставить randomstate, поэтому при повторном запуске программы результаты, вероятно, будут немного отличаться).\n",
    "\n",
    "Чтобы улучшить результат ещё сильнее, необходимо подобрать подходящие гиперпараметры: число циклов (epochs), размер контекстного окна (window_size), размерность векторного пространства (emb_size). Если увеличить объём текста или уменьшить объём словаря, это также положительно скажется на результате работы программы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом я заканчиваю свою работу. Надеюсь, вам понравилось.\n",
    "\n",
    "Спасибо за внимание и терпение!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Литература (точнее, ссылки)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://habr.com/ru/company/ods/blog/329410/\n",
    "\n",
    "https://neurohive.io/ru/osnovy-data-science/osnovy-nejronnyh-setej-algoritmy-obuchenie-funkcii-aktivacii-i-poteri/\n",
    "\n",
    "https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72\n",
    "\n",
    "https://deepnotes.io/softmax-crossentropy#cross-entropy-loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
